{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71532b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a new \"dataset\" with segmented MiniKinetics50 videos\n",
    "\n",
    "#1. Go through each video of dataset/mini50_clean_train.csv\n",
    "#2. Run YOLO + segmentation on it\n",
    "#3. If it could not find a person:\n",
    "    # Continue\n",
    "#4. If it did find a person:\n",
    "    # a) Add the video frames to the directory I just made\n",
    "        # in dataset/segmented_minikinetics50/train, create (if not created yet)\n",
    "        # or go to the directory corresponding to the label\n",
    "            # Create a new subdirectory with the youtube id only\n",
    "            # Add in the segmented images\n",
    "    # b) Write the row into the new csv\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/n/fs/visualai-scr/temp_LLP/ellie/slowfast_kinetics/sam2\")\n",
    "sys.path.insert(0, \"/n/fs/visualai-scr/temp_LLP/ellie/slowfast_kinetics/sam2\")\n",
    "# print(\"current working direcotry\", os.getcwd())\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "os.chdir(\"/n/fs/visualai-scr/temp_LLP/ellie/slowfast_kinetics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99988ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /u/ez9517/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2025-7-1 Python-3.10.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA L40, 45488MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "sam2_checkpoint = \"/n/fs/visualai-scr/temp_LLP/ellie/slowfast_kinetics/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "\n",
    "#model_cfg, expects a config path, not an absolute path\n",
    "os.chdir(\"/n/fs/visualai-scr/temp_LLP/ellie/slowfast_kinetics/sam2/sam2\")\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device='cpu')\n",
    "#TODO: reduce df to the max_videos if specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716ef143",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccff5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 label   youtube_id  time_start  time_end  \\\n",
      "0                 dribbling basketball  --4-0ihtnBU          58        68   \n",
      "1                       playing tennis  --56QUhyDQM         185       195   \n",
      "2                      climbing a rope  --EaS9P7ZdQ          13        23   \n",
      "3                       brushing teeth  --IPbe5ZMCI           2        12   \n",
      "4  skiing (not slalom or crosscountry)  --TBx-Spzis         148       158   \n",
      "\n",
      "   split                                          full_path  num_files  \n",
      "0  train  /n/fs/visualai-scr/Data/Kinetics_cvf/frames/tr...        113  \n",
      "1  train  /n/fs/visualai-scr/Data/Kinetics_cvf/frames/tr...        300  \n",
      "2  train  /n/fs/visualai-scr/Data/Kinetics_cvf/frames/tr...        300  \n",
      "3  train  /n/fs/visualai-scr/Data/Kinetics_cvf/frames/tr...        300  \n",
      "4  train  /n/fs/visualai-scr/Data/Kinetics_cvf/frames/tr...        300  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/mini50_clean_train.csv\")\n",
    "output_root = \"dataset/segmented_minikinetics50/train\"\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute indices for 8 and 32 evenly spaced frames\n",
    "def sample_indices(self, n, total_frames):\n",
    "    return [int(round(i * (total_frames - 1) / (n - 1) + 1)) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a row in csv, sample indices, run yolo & video segmentation on it\n",
    "def run_yolo_and_seg(row):\n",
    "    segmented_frames = [] #A list of PIL images containing all the segmented frames\n",
    "\n",
    "    video_path = row['full_path']\n",
    "    label = row['label']\n",
    "    youtube_id = row['youtube_idx']\n",
    "    num_files = row['num_files']\n",
    "    time_start = row['time_start']\n",
    "    time_end = row['time_end']\n",
    "    indices = sample_indices(32, num_files)\n",
    "    hasPerson = False\n",
    "\n",
    "\n",
    "    #Make a temporary directory storing video frames at certain indices\n",
    "    #Make a numpy array \"frames\" to store the video frames at certain indices\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    frames = []\n",
    "    for i in indices:\n",
    "        source = os.path.join(video_path, f\"{i:06d}.jpg\")\n",
    "        dest = os.path.join(temp_dir, f\"{i:06d}.jpg\")\n",
    "        os.symlink(source, dest)\n",
    "\n",
    "        img = Image.open(source).convert('RGB')  # Load as RGB\n",
    "        img_np = np.array(img) #convert to np image\n",
    "        frames.append(img_np)\n",
    "\n",
    "    video_np = np.stack(frames, axis=0)\n",
    "    print(\"VIDEO NP shape\", video_np.shape)\n",
    "\n",
    "    img0_np = video_np[0]\n",
    "    results = yolo(img0_np) #get bbox for the first frame\n",
    "    bboxes = results.xyxy[0]\n",
    "    person_bboxes = bboxes[bboxes[:, 5] == 0] # get bboxes for class == person only\n",
    "    person_bboxes = person_bboxes[:, :4].cpu().numpy()\n",
    "    \n",
    "    inference_state = predictor.init_state(video_path=temp_dir)\n",
    "\n",
    "    #If a person was detected, then segment out the person\n",
    "    if person_bboxes.shape[0] >= 1:\n",
    "        hasPerson = True\n",
    "        box = np.array(person_bboxes[0], dtype=np.float32)\n",
    "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=0,\n",
    "            obj_id=1,\n",
    "            box=box,\n",
    "        )\n",
    "\n",
    "        #Go through each video frame, propogate the segmentation, calculate the binary mask\n",
    "        for out_frame_idx, _, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "            binarymask = (out_mask_logits.squeeze(0).squeeze(0)) > 0.0 #binary mask, where 1 is person, 0 isnot\n",
    "            binarymask = binarymask.unsqueeze(2).to(device)\n",
    "\n",
    "            img = torch.from_numpy(video_np[out_frame_idx]).to(device)\n",
    "            seg_img = binarymask * img\n",
    "            seg_img_np = seg_img.cpu().numpy()\n",
    "            if seg_img_np.shape[0] == 3:  # (3, H, W) -> (H, W, 3)\n",
    "                seg_img_np = np.transpose(seg_img_np, (1, 2, 0))\n",
    "\n",
    "            segmented_frames.append(seg_img_np)\n",
    "\n",
    "    return segmented_frames, hasPerson         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "            #Save the segmented image to the new dataset directory path\n",
    "            label_path = f\"/n/fs/visualai-scr/temp_LLP/ellie/slowfast_kinetics/dataset/segmented_minikinetics50/train/{label}\"\n",
    "            os.makedirs(label_path, exist_ok=True)\n",
    "            vid_path = os.makedirs(os.join(label_path, f\"{youtube_id}_{time_start:06d}_{time_end:06d}\"))\n",
    "            os.makedirs(vid_path, exist_ok=True)\n",
    "            Image.fromarray(seg_img_np).save(os.join(vid_path, f\"{out_frame_idx:06d}.jpg\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37bc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    video_path = row['full_path']\n",
    "    label = row['label']\n",
    "    youtube_id = row['youtube_idx']\n",
    "    num_files = row['num_files']\n",
    "\n",
    "    hasPerson = run_seg_and_save(row)\n",
    "\n",
    "    if hasPerson == True:\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellie_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
