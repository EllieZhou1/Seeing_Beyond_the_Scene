{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2135a30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/visualai-scr/temp_LLP/ellie/miniconda3/envs/ellie_env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL.Image as Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e3e926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0667cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/n/fs/visualai-scr/Data/HAT2\"\n",
    "# original_path = os.path.join(base_dir, \"original\", \"archery\", \"0S-P4lr_c7s_000022_000032\", \"000007.jpg\")\n",
    "# seg_path = os.path.join(base_dir, \"seg\", \"archery\", \"0S-P4lr_c7s_000022_000032\", \"000007.png\")\n",
    "# bg_path = os.path.join(base_dir, \"inpaint\", \"archery\", \"0S-P4lr_c7s_000022_000032\", \"000007.jpg\")\n",
    "\n",
    "\n",
    "correct_label = \"juggling balls\"\n",
    "youtube_id = \"C-AOi2aAPvc_000041_000051\"\n",
    "frame_num = 50\n",
    "\n",
    "original_path = os.path.join(base_dir, \"original\", correct_label, youtube_id, f\"{frame_num:06d}.jpg\")\n",
    "seg_path = os.path.join(base_dir, \"seg\", correct_label, youtube_id, f\"{frame_num:06d}.png\")\n",
    "original_path = os.path.join(base_dir, \"inpaint\", correct_label, youtube_id, f\"{frame_num:06d}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11e77d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load CLIP model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/clip-vit-base-patch32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded CLIP model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/n/fs/visualai-scr/temp_LLP/ellie/miniconda3/envs/ellie_env2/lib/python3.10/site-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/n/fs/visualai-scr/temp_LLP/ellie/miniconda3/envs/ellie_env2/lib/python3.10/site-packages/transformers/modeling_utils.py:4839\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4830\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4832\u001b[0m     (\n\u001b[1;32m   4833\u001b[0m         model,\n\u001b[1;32m   4834\u001b[0m         missing_keys,\n\u001b[1;32m   4835\u001b[0m         unexpected_keys,\n\u001b[1;32m   4836\u001b[0m         mismatched_keys,\n\u001b[1;32m   4837\u001b[0m         offload_index,\n\u001b[1;32m   4838\u001b[0m         error_msgs,\n\u001b[0;32m-> 4839\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4857\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4858\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m/n/fs/visualai-scr/temp_LLP/ellie/miniconda3/envs/ellie_env2/lib/python3.10/site-packages/transformers/modeling_utils.py:5105\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5102\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   5103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5104\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m-> 5105\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   5106\u001b[0m     )\n\u001b[1;32m   5108\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[1;32m   5109\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[0;32m/n/fs/visualai-scr/temp_LLP/ellie/miniconda3/envs/ellie_env2/lib/python3.10/site-packages/transformers/modeling_utils.py:574\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_file, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m map_location \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_zipfile(checkpoint_file):\n\u001b[1;32m    573\u001b[0m         extra_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/n/fs/visualai-scr/temp_LLP/ellie/miniconda3/envs/ellie_env2/lib/python3.10/site-packages/torch/serialization.py:1516\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/n/fs/visualai-scr/temp_LLP/ellie/miniconda3/envs/ellie_env2/lib/python3.10/site-packages/torch/serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m/n/fs/visualai-scr/temp_LLP/ellie/miniconda3/envs/ellie_env2/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:403\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    401\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 403\u001b[0m     func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _get_allowed_globals()\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _get_user_allowed_globals()\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    405\u001b[0m ):\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to call reduce for unrecognized function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    409\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "print(\"Loaded CLIP model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9aeaf900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_action(original_path, seg_path, background_only_path, \n",
    "                      all_labels, correct_label):\n",
    "    \"\"\"\n",
    "    Test bias for one action with 3 image versions\n",
    "    \n",
    "    Args:\n",
    "        original_path: path to original image\n",
    "        human_only_path: path to human-only image  \n",
    "        background_only_path: path to background-only image\n",
    "        all_labels: list of all 50 action labels\n",
    "        correct_label: the correct label for this action\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with rankings and scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the 3 images\n",
    "    images = []\n",
    "\n",
    "    original = np.array(Image.open(original_path).convert('RGB'))  \n",
    "    seg = np.array(Image.open(seg_path).convert('L'))  # Binary mask\n",
    "    inpaint = np.array(Image.open(background_only_path).convert('RGB')) \n",
    "\n",
    "    seg_norm = seg.astype(np.float32) / 255.0\n",
    "    # print(\"seg min and max\", seg_norm.min(), seg_norm.max())\n",
    "\n",
    "    mask_3d = np.stack([seg_norm, seg_norm, seg_norm], axis=2)\n",
    "    human_only = (mask_3d * original).astype(np.uint8)\n",
    "\n",
    "    # print(\"original min and max\", original.min(), original.max())\n",
    "    # print(\"inpaint min and max\", inpaint.min(), inpaint.max())\n",
    "    # print(\"seg_norm min and max\", seg_norm.min(), seg_norm.max())\n",
    "    # print(\"human only min and max\", human_only.min(), human_only.max())\n",
    "\n",
    "    # plt.imshow(human_only)\n",
    "    # plt.axis('off')  # hides axis\n",
    "    # plt.show()\n",
    "\n",
    "    images.append(original)\n",
    "    images.append(human_only)\n",
    "    images.append(inpaint)\n",
    "    \n",
    "    # Process images and text together\n",
    "    inputs = processor(text=all_labels, images=images, return_tensors=\"pt\", \n",
    "                      padding=True, do_convert_rgb=False)\n",
    "    \n",
    "    # Get CLIP predictions\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # shape: (3, num_labels)\n",
    "    probs = logits_per_image.softmax(dim=1)  # convert to probabilities\n",
    "    \n",
    "    # Find index of correct label\n",
    "    correct_idx = all_labels.index(correct_label)\n",
    "    \n",
    "    results = {}\n",
    "    image_types = ['original', 'human_only', 'background_only']\n",
    "    \n",
    "    for i, img_type in enumerate(image_types):\n",
    "        # Get probability scores for this image\n",
    "        scores = probs[i].detach().numpy()\n",
    "        \n",
    "        # Get full ranking order (indices sorted by score, descending)\n",
    "        ranking_indices = np.argsort(scores)[::-1]\n",
    "        \n",
    "        # Create ranked list of actions with scores\n",
    "        ranked_actions = []\n",
    "        for rank, idx in enumerate(ranking_indices):\n",
    "            ranked_actions.append({\n",
    "                'rank': rank + 1,\n",
    "                'action': all_labels[idx],\n",
    "                'score': float(scores[idx])\n",
    "            })\n",
    "        \n",
    "        # Find where correct label ranks\n",
    "        correct_ranking = np.where(ranking_indices == correct_idx)[0][0] + 1\n",
    "        correct_score = scores[correct_idx].item()\n",
    "        \n",
    "        results[img_type] = {\n",
    "            'correct_ranking': int(correct_ranking), #The ranking of the correct text label among all labels\n",
    "            'correct_score': float(correct_score), #Similarity score of the img & the correct text label\n",
    "            'full_rankings': ranked_actions, #Full ranking of all 50 labels, ranked by similarity\n",
    "            'top5': correct_ranking <= 5 #Whether or not the correct ranking was in the top 5 or not\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56dff158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for juggling balls:\n",
      "\n",
      "ORIGINAL:\n",
      "  Correct action rank: 2/50 (score: 0.432)\n",
      "  Top 5 predictions:\n",
      "        1. juggling soccer ball (0.449)\n",
      "        2. juggling balls (0.432)\n",
      "        3. cleaning windows (0.031)\n",
      "        4. shooting goal (soccer) (0.019)\n",
      "        5. catching or throwing frisbee (0.012)\n",
      "\n",
      "HUMAN_ONLY:\n",
      "  Correct action rank: 8/50 (score: 0.035)\n",
      "  Top 5 predictions:\n",
      "        1. punching person (boxing) (0.173)\n",
      "        2. bowling (0.083)\n",
      "        3. sword fighting (0.071)\n",
      "        4. playing saxophone (0.067)\n",
      "        5. skiing slalom (0.059)\n",
      "\n",
      "BACKGROUND_ONLY:\n",
      "  Correct action rank: 7/50 (score: 0.031)\n",
      "  Top 5 predictions:\n",
      "        1. shooting goal (soccer) (0.266)\n",
      "        2. golf driving (0.228)\n",
      "        3. archery (0.202)\n",
      "        4. juggling soccer ball (0.071)\n",
      "        5. catching or throwing frisbee (0.045)\n",
      "\n",
      "âœ… Good: Background-only image doesn't rank highly\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Your 50 action labels\n",
    "    all_labels = ['playing guitar', 'bowling', 'playing saxophone', 'brushing teeth', \n",
    "                    'playing basketball', 'tying tie', 'skiing slalom', 'brushing hair', \n",
    "                    'punching person (boxing)', 'playing accordion', 'archery', \n",
    "                    'catching or throwing frisbee', 'drinking', 'reading book', \n",
    "                    'eating ice cream', 'flying kite', 'sweeping floor', \n",
    "                    'walking the dog', 'skipping rope', 'clean and jerk', \n",
    "                    'eating cake', 'catching or throwing baseball', \n",
    "                    'skiing (not slalom or crosscountry)', 'juggling soccer ball', \n",
    "                    'deadlifting', 'driving car', 'cleaning windows', 'shooting basketball', \n",
    "                    'canoeing or kayaking', 'surfing water', 'playing volleyball', 'opening bottle', \n",
    "                    'playing piano', 'writing', 'dribbling basketball', 'reading newspaper', 'playing violin', \n",
    "                    'juggling balls', 'playing trumpet', 'smoking', 'shooting goal (soccer)', 'hitting baseball', \n",
    "                    'sword fighting', 'climbing ladder', 'playing bass guitar', 'playing tennis', 'climbing a rope', \n",
    "                    'golf driving', 'hurdling', 'dunking basketball']\n",
    "    \n",
    "    # Test one action\n",
    "    result = test_single_action(\n",
    "        original_path=original_path,\n",
    "        seg_path=seg_path, \n",
    "        background_only_path=bg_path,\n",
    "        all_labels=all_labels,\n",
    "        correct_label=correct_label\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Results for {correct_label}:\")\n",
    "    \n",
    "    for img_type in ['original', 'human_only', 'background_only']:\n",
    "        print(f\"\\n{img_type.upper()}:\")\n",
    "        print(f\"  Correct action rank: {result[img_type]['correct_ranking']}/50 (score: {result[img_type]['correct_score']:.3f})\")\n",
    "        print(f\"  Top 5 predictions:\")\n",
    "        for item in result[img_type]['full_rankings'][:5]:\n",
    "            marker = \"ðŸ‘‰\" if item['action'] == \"playing tennis\" else \"  \"\n",
    "            print(f\"    {marker} {item['rank']:2d}. {item['action']} ({item['score']:.3f})\")\n",
    "    \n",
    "    # Check for bias\n",
    "    if result['background_only']['top5']:\n",
    "        print(\"\\nâš ï¸  POTENTIAL BIAS: Background-only image ranks in top 5!\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Good: Background-only image doesn't rank highly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellie_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
